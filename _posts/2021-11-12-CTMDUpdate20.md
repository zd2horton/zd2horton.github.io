---
layout: post
title: Creative Technologies Master Dissertation Update 20
date: 2021-11-12
excerpt: Creating new configurations, and observing results.
tags: [CTMD, post, CTMDpost]
CTMDpost: true
category: post
published: true
comments: true
---
With the further development of the report, focus has returned to implementation and consequently an altered configuration file was created in order to potentially garner more positive results from gap jumping. Various values were altered for a number of reasons to aid with learning, detailed below.

- Batch size was notably decreased, but still kept large in order to provide more experiences in each iteration of gradient descent for the learning method.

- Learning rate was decreased as to give more stable training and in the case of reward not consistently increasing. 

- Beta was decreased in order to reduce randomness and give a more focused training. 

- Hidden Units were increased in the case of the scenario being more complex than other setups that are without obstacles. 

- Gamma was decreased in order to frame the scenario in terms of the current reward.

- Time Horizon was decreased, as doing such seemingly gives a more concentrated and focused estimate. 

Default config (left), compared to the newer config (right):
<a href="https://i.imgur.com/6UT1wDY.png"><img src="https://i.imgur.com/6UT1wDY.png"></a>


After the variables had been altered for more considerate and focused values, step learning proved notably successfully alongside a far more instantly successful gap session afterwards. However, though a final reward of 1.00 was given to the resulting reward and training had visibly succeeded, the model completely misguided the agent and resulted in constant failures. Other similar runs altering gap position or size were undertaken and initialised with the successful step learning, still leading to the same outcome and thus verifying that the agent was unwilling to learn from prior mistakes and did not vary its attempts with the obstacle. Likewise, an alteration to the newer configuration file was made, in order to try and solve this. 

- Batch size was increased to the discrete maximum value, in order to increase experiences in iterations. Due to the increase, number of epochs (passes through experience buffer with gradient descent optimisation) was increased from 3 to 4.

- Buffer size was somewhat increased from default as to assure more experiences were collected before the model was updated, giving it more rounded out learning. Beta was increased from its previous value alongside this. 

- Learning rate was increased, surpassing the default value as to attempt more guessed tries from the agent rather than sticking to normal tries.

- Gamma was decreased, as it was believed that the agent should have acted more in terms of long term reward preparation.

- As well as this, time horizon was increased back to a default of 64 as to give a slightly more random response.

The previously made config (left), compared to the newer more expanded config (right):
<a href="https://i.imgur.com/PSdIQhP.png"><img src="https://i.imgur.com/PSdIQhP.png"></a>

However, despite marginally improved results with step tests, gap tests had gathered the same outcomes. As too large a number of variables have been altered, subsequent runs featuring same conditions will progress through default values and, if more successful, will be analysed value by value to potentially discover which variables require tuning.


<figure class="video_container">
  <video width="640" height="360" controls="true" allowfullscreen="true">
    <source src="https://zd2horton.github.io/assets/video/10th Custom Config.mp4" type="video/mp4">
  </video>
</figure>

<figure class="video_container">
  <video width="640" height="360" controls="true" allowfullscreen="true">
    <source src="https://zd2horton.github.io/assets/video/11th Alter Config.mp4" type="video/mp4">
  </video>
</figure>